# -*- coding: utf-8 -*-
"""A4_<SM24MTECH11003>.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15KAtlzBPcRRS_LtmPLXMQMdSrrZW5NXW

NAME- PRINCE VERMA

Roll number:- SM24MTECH11003

# **Autoencoders**

Prepare the Dataset
"""

import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Transform to convert images to tensor
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])

# Download the FashionMNIST dataset
train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)

# DataLoader for batching
train_loader = DataLoader(train_data, batch_size=128, shuffle=True)
test_loader = DataLoader(test_data, batch_size=128, shuffle=False)

"""Data Sample

"""

import matplotlib.pyplot as plt

# Display the first 5 images and their labels from the train dataset
for i in range(5):
    image, label = train_data[i]  # Get the i-th image and its label from the train dataset
    plt.imshow(image.squeeze(), cmap='gray')  # Squeeze to remove unnecessary dimension
    plt.title(f"Train Label: {label}")  # Show the label
    plt.axis('off')  # Hide the axes
    plt.show()
print("this is the images from train data")


# Display the first 5 images and their labels from the test dataset
for i in range(5):
    image, label = test_data[i]  # Get the i-th image and its label from the test dataset
    plt.imshow(image.squeeze(), cmap='gray')  # Squeeze to remove unnecessary dimension
    plt.title(f"Test Label: {label}")  # Show the label
    plt.axis('off')  # Hide the axes
    plt.show()
print("this is the images from test data")

# Get the first image and check its dimensions
image, label = train_data[0]  # Get the first image and its label
print(f"Image dimensions: {image.shape}")

"""(A)

Define the Autoencoder Model
"""

import torch.nn as nn
import torch.optim as optim
# Define the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
class Autoencoder(nn.Module):
    def __init__(self, latent_dim):
        super(Autoencoder, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 128),
            nn.ReLU(),
            nn.Linear(128, latent_dim)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 28*28),
            nn.Sigmoid(),  # Sigmoid to output values in [0, 1] range
            nn.Unflatten(1, (1, 28, 28))  # Reshape to image format
        )

    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed

"""Train the Autoencoder"""

import torch
import torch.optim as optim
import torch.nn as nn
import matplotlib.pyplot as plt

# Function to train the Autoencoder and store loss for each epoch
def train_autoencoder(model, train_loader, criterion, optimizer, epochs=10):
    model.train()
    epoch_losses = []  # List to store the loss for each epoch
    for epoch in range(epochs):
        total_loss = 0
        for data, _ in train_loader:
            data = data.to(device)  # Move data to device (CPU/GPU)
            optimizer.zero_grad()  # Zero the gradients
            reconstructed = model(data)  # Reconstruct the input data
            loss = criterion(reconstructed, data)  # Calculate the loss (MSE)
            loss.backward()  # Backpropagate the error
            optimizer.step()  # Update the model's parameters
            total_loss += loss.item()  # Accumulate the loss for this batch
        avg_loss = total_loss / len(train_loader)  # Average loss for this epoch
        epoch_losses.append(avg_loss)  # Store loss for this epoch
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}")

    return epoch_losses  # Return the list of losses for plotting

# Initialize and train models with different latent dimensions
latent_dims = [16, 48]
models = {latent_dim: Autoencoder(latent_dim).to(device) for latent_dim in latent_dims}

# Criterion and Optimizer
criterion = nn.MSELoss()  # MSE loss function for reconstruction error
optimizer = {latent_dim: optim.Adam(models[latent_dim].parameters()) for latent_dim in latent_dims}

# Store losses for each model
all_losses = {}

# Train each model and store losses
for latent_dim in latent_dims:
    print(f"Training Autoencoder with latent dimension {latent_dim}")
    losses = train_autoencoder(models[latent_dim], train_loader, criterion, optimizer[latent_dim], epochs=10)
    all_losses[latent_dim] = losses  # Store the losses for each latent dimension

# Plot the losses for both latent dimensions
plt.figure(figsize=(10, 6))
for latent_dim in latent_dims:
    plt.plot(range(1, 11), all_losses[latent_dim], label=f'Latent Dim = {latent_dim}')

plt.title('Training Loss per Epoch for Different Latent Dimensions')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True)
plt.show()

# Function to evaluate reconstruction error (MSE loss) on the test set
def evaluate_reconstruction_error(model, data_loader, criterion):
    model.eval()  # Set model to evaluation mode
    total_loss = 0  # Initialize the loss accumulator
    with torch.no_grad():  # No gradients needed during evaluation
        for data, _ in data_loader:
            data = data.to(device)  # Move data to the correct device
            reconstructed = model(data)  # Get the reconstructed image
            loss = criterion(reconstructed, data)  # Calculate MSE loss
            total_loss += loss.item()  # Accumulate loss
    return total_loss / len(data_loader)  # Return average loss

# After training, evaluate and print reconstruction errors on the test set
for latent_dim in latent_dims:
    test_loss = evaluate_reconstruction_error(models[latent_dim], test_loader, criterion)
    print(f"Reconstruction error (MSE) for latent dimension {latent_dim} on test set: {test_loss:.4f}")

"""

Evaluate and Visualize the Results"""

import matplotlib.pyplot as plt

def visualize_reconstructions_separately(model, data_loader, latent_dim):
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        data, _ = next(iter(data_loader))  # Get a batch of data
        data = data.to(device)  # Send data to the device (GPU/CPU)

        # Reconstruct the data using the model
        reconstructed = model(data)

        # Move data and reconstructed images back to CPU for plotting
        data = data.cpu()
        reconstructed = reconstructed.cpu()

        # Show original images
        fig, axes = plt.subplots(1, 8, figsize=(12, 4))
        for i in range(8):
            axes[i].imshow(data[i].squeeze(), cmap="gray")  # Show original image
            axes[i].set_title(f"Original {i+1}")
            axes[i].axis('off')
        plt.suptitle(f"Original Images for Latent Dim {latent_dim}", fontsize=14)
        plt.tight_layout()
        plt.subplots_adjust(top=0.85)  # Adjust space for the suptitle
        plt.show()

        # Show reconstructed images
        fig, axes = plt.subplots(1, 8, figsize=(12, 4))
        for i in range(8):
            axes[i].imshow(reconstructed[i].squeeze(), cmap="gray")  # Show reconstructed image
            axes[i].set_title(f"Reconstructed {i+1}")
            axes[i].axis('off')
        plt.suptitle(f"Reconstructed Images for Latent Dim {latent_dim}", fontsize=14)
        plt.tight_layout()
        plt.subplots_adjust(top=0.85)  # Adjust space for the suptitle
        plt.show()

# Visualize original and reconstructed images for both latent dimensions
for latent_dim in latent_dims:
    print(f"Visualizing original and reconstructed images for latent dimension {latent_dim}")
    visualize_reconstructions_separately(models[latent_dim], test_loader, latent_dim)

"""The autoencoder with latent dimension 48 should produce more accurate and visually appealing reconstructions with sharper details.

The autoencoder with latent dimension 16 will likely have blurry or less detailed reconstructions, as it struggles to capture all the nuances of the images due to its limited capacity.

(b)

Interpolate in Latent Space
"""

import torch
import matplotlib.pyplot as plt

def interpolate_in_latent_space_for_both_dims(model_16, model_48, data_loader, num_pairs=4, num_interpolations=5):
    model_16.eval()
    model_48.eval()

    with torch.no_grad():
        data, _ = next(iter(data_loader))  # Get a batch of data
        data = data[:num_pairs*2].to(device)  # Select pairs of images (4 pairs, total 8 images)

        # Get latent representations of these images for both latent dimensions
        latent_representations_16 = model_16.encoder(data).cpu()
        latent_representations_48 = model_48.encoder(data).cpu()

        # Interpolate between each pair of images
        for i in range(0, num_pairs*2, 2):  # Iterate over pairs (i, i+1)
            latent_start_16 = latent_representations_16[i]
            latent_end_16 = latent_representations_16[i+1]
            latent_start_48 = latent_representations_48[i]
            latent_end_48 = latent_representations_48[i+1]

            # Create a plot for each image pair
            fig, axes = plt.subplots(2, num_interpolations, figsize=(num_interpolations*2, 4))

            # First, show interpolation for latent dimension 16
            for j in range(num_interpolations):  # Interpolate between the two latent representations
                t = j / (num_interpolations - 1)  # t goes from 0 to 1
                interpolation_16 = latent_start_16 + t * (latent_end_16 - latent_start_16)

                # Add batch dimension to the interpolation
                interpolation_16 = interpolation_16.unsqueeze(0)

                # Decode the interpolated latent point to reconstruct the image
                reconstructed_16 = model_16.decoder(interpolation_16.to(device)).cpu()

                # Reshape the output to 28x28 image
                reconstructed_16 = reconstructed_16.view(28, 28)

                # Display the image for latent dim 16
                axes[0, j].imshow(reconstructed_16, cmap="gray")
                axes[0, j].axis('off')

                # Label the images for latent dim 16
                if j == 0:
                    axes[0, j].set_title("Original Image")  # First image is the original
                elif j == num_interpolations - 1:
                    axes[0, j].set_title("Second Image")  # Last image is the second image
                else:
                    axes[0, j].set_title(f"Interpolated {j}")  # Other images are interpolated

            # Now, show interpolation for latent dimension 48
            for j in range(num_interpolations):  # Interpolate between the two latent representations
                t = j / (num_interpolations - 1)  # t goes from 0 to 1
                interpolation_48 = latent_start_48 + t * (latent_end_48 - latent_start_48)

                # Add batch dimension to the interpolation
                interpolation_48 = interpolation_48.unsqueeze(0)

                # Decode the interpolated latent point to reconstruct the image
                reconstructed_48 = model_48.decoder(interpolation_48.to(device)).cpu()

                # Reshape the output to 28x28 image
                reconstructed_48 = reconstructed_48.view(28, 28)

                # Display the image for latent dim 48
                axes[1, j].imshow(reconstructed_48, cmap="gray")
                axes[1, j].axis('off')

                # Label the images for latent dim 48
                if j == 0:
                    axes[1, j].set_title("Original Image")  # First image is the original
                elif j == num_interpolations - 1:
                    axes[1, j].set_title("Second Image")  # Last image is the second image
                else:
                    axes[1, j].set_title(f"Interpolated {j}")  # Other images are interpolated

            # Set the titles for each row using fig.text (this avoids overlap issues)
            fig.text(0.5, 0.98, 'Latent Dimension 16', ha='center', va='center', fontsize=14)
            fig.text(0.5, 0.46, 'Latent Dimension 48', ha='center', va='center', fontsize=14)

            # Title for the plot
            plt.suptitle(f"Image Pair {i//2 + 1}", fontsize=14)
            plt.tight_layout()
            plt.subplots_adjust(top=0.85, hspace=0.5)  # Adjust spacing for the suptitle and row titles
            plt.show()

# Example usage:
interpolate_in_latent_space_for_both_dims(models[16], models[48], test_loader)

"""What trends do you notice as you move between
samples?

1-Smooth Transitions:

As you interpolate between the latent space representations of two images, the resulting images will smoothly transition from one to the other. This means that at the start of the interpolation, the image will look like the first sample, and as you move towards the end of the interpolation, it will gradually take on characteristics of the second sample.

2-Gradual Morphing of Features:

In the latent space, the features of the two images gradually combine. For example:

Shirt → Pants: The intermediate images may show parts of a shirt mixed with parts of pants, like a half-shirt, half-pants or a shirt with pant-like features.

Hybridization of Features: As the interpolation progresses, you'll notice images where the features from both categories (like the silhouette of a shirt and the legs of pants) start blending more clearly.

3-Latent Dimension Impact:

**For a latent dimension of 16**: The interpolation might appear less smooth and more distorted or blurry. This is because the lower-dimensional latent space doesn't capture all the fine details of the images. The transitions between the images may look more abrupt, and the intermediate images might not fully retain the structure of the original images.

For a latent dimension of 48 **bold text**: The transitions between the images tend to be smoother and more detailed. The latent space with more dimensions can capture more information, resulting in clearer and more natural-looking interpolations. The intermediate images will look more coherent, with better retention of the essential features from both the start and end images.

# 2. Variational Autoencoder(VAE):

Define the VAE Model
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np

# Define the VAE model

class VAE(nn.Module):
    def __init__(self, latent_dim=12):
        super(VAE, self).__init__()

        # Encoder
        self.fc1 = nn.Linear(28*28, 400)
        self.fc2_mean = nn.Linear(400, latent_dim)  # Mean of the latent distribution
        self.fc2_log_var = nn.Linear(400, latent_dim)  # Log variance of the latent distribution

        # Decoder
        self.fc3 = nn.Linear(latent_dim, 400)
        self.fc4 = nn.Linear(400, 28*28)

    def encode(self, x):
        h1 = torch.relu(self.fc1(x))
        mean = self.fc2_mean(h1)
        log_var = self.fc2_log_var(h1)
        return mean, log_var

    def reparameterize(self, mean, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        z = mean + std * eps  # Reparameterization trick
        return z

    def decode(self, z):
        h3 = torch.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))

    def forward(self, x):
        mean, log_var = self.encode(x.view(-1, 28*28))
        z = self.reparameterize(mean, log_var)
        recon_x = self.decode(z)
        return recon_x, mean, log_var

    def loss_function(self, recon_x, x, mean, log_var):
        BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 28*28), reduction='sum')
        # KL divergence
        # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
        KL = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())
        return BCE + KL

"""Train the VAE Model"""

# Data preparation
transform = transforms.Compose([transforms.ToTensor()])  # Remove normalization to get values between [0, 1]
train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=False)

# Initialize model and optimizer
latent_dim = 12
model = VAE(latent_dim).to(device)  # Move model to the chosen device (GPU or CPU)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# Training loop
def train(model, train_loader, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        train_loss = 0
        for data, _ in train_loader:
            data = data.to(device)  # Move data to the chosen device (GPU or CPU)
            optimizer.zero_grad()
            recon_data, mean, log_var = model(data)
            loss = model.loss_function(recon_data, data, mean, log_var)
            loss.backward()
            train_loss += loss.item()
            optimizer.step()
        print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss / len(train_loader.dataset)}')

train(model, train_loader, optimizer)

"""Generate New Images from the Learned Distribution"""

import torchvision
def generate_images(model, num_samples=50):
    model.eval()
    with torch.no_grad():
        # Sample from a standard normal distribution and decode
        z = torch.randn(num_samples, latent_dim)  # No need to move to GPU for CPU version
        z = z.to(device)  # Move z to the correct device (CPU in this case)
        generated_images = model.decode(z)  # Decode the sampled z to generate images

        # Visualize generated images
        grid = torchvision.utils.make_grid(generated_images.view(num_samples, 1, 28, 28), nrow=10)
        plt.figure(figsize=(10, 10))
        plt.imshow(grid.permute(1, 2, 0))  # Permute for displaying in a grid format
        plt.axis('off')
        plt.show()

# Call the function to generate images (will run on CPU now)
generate_images(model)

"""Bonus - Tweak a Latent Variable and Observe Its Effect"""

def tweak_latent_variable_and_generate(model, latent_dim=12):
    model.eval()
    with torch.no_grad():
        # Generate a random latent vector (12-dimensional)
        z = torch.randn(1, latent_dim).to(device)

        # Tweak one latent variable (e.g., shift the first dimension)
        z[0, 0] += 2.0  # Shift the first latent variable by a value (2.0 in this case)

        # Decode the modified latent vector to generate the new image
        generated_image = model.decode(z).cpu()

        # Visualize the generated image
        plt.imshow(generated_image.view(28, 28), cmap='gray')
        plt.axis('off')
        plt.title(f"Modified Latent Variable (dim 0)")
        plt.show()

# Call the function to tweak a latent variable and generate a new sample
tweak_latent_variable_and_generate(model)

"""# **Comment on the Diversity and Quality of the Generated Samples**

Diversity: After generating 50 images from the learned latent space, the samples include a wide variety of clothing items such as shirts, pants, dresses, shoes, and more. Each image looks distinct from the others, demonstrating good diversity in the generated samples.

Quality: The quality of the generated images is quite good. The images are clear and resemble actual items of clothing from the Fashion-MNIST dataset. While there may be some minor imperfections or blurriness, the majority of the images are easily recognizable as shirts, pants, or shoes.

Latent Space Interpolation: When tweaking latent variables, it's clear that certain dimensions control specific aspects of the clothing. For instance, tweaking one latent dimension changes the clothing type, while another may modify the color or style. This indicates that the VAE has learned a structured and interpretable latent space.
"""

def visualize_latent_and_image_for_all_dimensions(model, latent_dim=12):
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        # Step 1: Generate a random latent vector (before tweak)
        z = torch.randn(1, latent_dim).to(device)  # Sample a latent vector from standard normal distribution

        # Generate original image from latent vector (before tweak)
        generated_image_before = model.decode(z).cpu()  # Generate image before tweak

        # Visualize original latent vector and corresponding image
        print("Original Latent Vector (Before Tweak):")
        print(z)  # Print the original latent vector
        plt.imshow(generated_image_before.view(28, 28), cmap='gray')
        plt.axis('off')
        plt.title("Original Image (Before Tweak)")
        plt.show()

        # Step 2: Loop through each latent dimension (from 0 to 11) and tweak them one by one
        for i in range(latent_dim):
            # Copy the original z for each dimension tweak
            z_copy = z.clone()

            # Tweak the i-th latent dimension (adding 2.0 for visibility of change)
            z_copy[0, i] += 2.0  # Modify the i-th latent dimension

            # Generate image from modified latent vector
            generated_image_after = model.decode(z_copy).cpu()  # Generate image after tweak

            # Visualize modified latent vector and corresponding image
            print(f"Modified Latent Vector at z[0, {i}] (After Tweak):")
            print(z_copy)  # Print the modified latent vector
            plt.imshow(generated_image_after.view(28, 28), cmap='gray')
            plt.axis('off')
            plt.title(f"Modified Image (After Tweak of z[0, {i}])")
            plt.show()

# Call the function to visualize the latent vector and image before and after tweaking each latent dimension
visualize_latent_and_image_for_all_dimensions(model)
